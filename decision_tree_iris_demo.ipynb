{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "velvet-metropolitan",
   "metadata": {},
   "source": [
    "# Decision Trees for You and Me!\n",
    "\n",
    "## Multi-Class Classification with the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-communist",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, stratify_by=None):\n",
    "    \"\"\"\n",
    "    Crude train, validate, test split\n",
    "    To stratify, send in a column name for the stratify_by argument\n",
    "    \"\"\"\n",
    "\n",
    "    if stratify_by == None:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=123)\n",
    "    else:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=123, stratify=df[stratify_by])\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train[stratify_by])\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire Stage\n",
    "df = data('iris')\n",
    "\n",
    "# Prep\n",
    "df.columns = [col.lower().replace('.', '_') for col in df]\n",
    "\n",
    "train, validate, test = split(df, stratify_by=\"species\")\n",
    "\n",
    "print(train.shape, validate.shape, test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our X inputs and y target variable for each split\n",
    "X_train = train.drop(columns=['species'])\n",
    "y_train = train.species\n",
    "\n",
    "X_validate = validate.drop(columns=['species'])\n",
    "y_validate = validate.species\n",
    "\n",
    "X_test = test.drop(columns=['species'])\n",
    "y_test = test.species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-neutral",
   "metadata": {},
   "source": [
    "This notebook is skipping the Exploration stage, because we have already explored this data.\n",
    "\n",
    "Remember that one of the deliverables from our Exploration stage is narrowing down which features we'll use to model. \n",
    "\n",
    "For this example, it's pretty direct:\n",
    "- Our target variable is species\n",
    "- Our input variables are sepal and petal length and width\n",
    "\n",
    "Onto the modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a blank, new Decision Tree model\n",
    "# Be sure to set the max_depth argument\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train our model on the training data\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To programmatically specify the output class labels\n",
    "# Visualize the model so iut can explain itself!\n",
    "# dataframe.target_variable.unique() then sort the array\n",
    "\n",
    "dot_data = export_graphviz(clf, feature_names= X_train.columns, rounded=True, filled=True, out_file=None, class_names=clf.classes_)\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render('iris_decision_tree', view=True, format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll make a set of predictions using this trained model\n",
    "y_pred = clf.predict(X_train)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the probabilities for each class\n",
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "y_pred_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-palestine",
   "metadata": {},
   "source": [
    "## Takeaways so far\n",
    "- 96% accuracy on training data. \n",
    "- This specific model is pretty good at predicting setosa on the train data\n",
    "- But how does this model perform on out-of-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate this model on out-of-sample data\n",
    "print('Accuracy of Decision Tree classifier on validate set: {:.2f}'\n",
    "     .format(clf.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the classification model trained on train data to make predictions on validate data\n",
    "y_pred = clf.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual y values from validate to predictions based on X_validate\n",
    "print(classification_report(y_validate, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-pride",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "- Train a single model and see if it beats the baseline? If so, keep going. If not, maybe baseline is OK\n",
    "- Evaluate that single model on validate dataset to see how well it performs on out of sample data\n",
    "- We might then make or tweak a few other models (with different features and different hyperparameter arguments)\n",
    "- We'll evaluate our handful of models on validate, then pick the best performing one.\n",
    "- Once we've picked our shining model, then we'll evaluate its performance on the TEST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orange is setosa\n",
    "train[train.petal_length <= 2.5].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_setosas = train[train.petal_length > 2.5]\n",
    "not_setosas.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriscolor is green\n",
    "versicolor = not_setosas[not_setosas.petal_length <= 4.75]\n",
    "versicolor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# purple is virginica\n",
    "mostly_virginica = not_setosas[not_setosas.petal_length > 4.75]\n",
    "mostly_virginica.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually check out the decision rules from the trained model on validate\n",
    "validate[validate.petal_length <= 2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_or_v = validate[validate.petal_length > 2.5]\n",
    "v_or_v.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_or_v[v_or_v.petal_length <= 4.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-combat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
